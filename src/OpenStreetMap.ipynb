{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OpenStreetMap (Data Wrangling with MongoDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toronto Area\n",
    "Here are the coordinates I used to analyze an area around Toronto Downtown:  \n",
    "(node(43.6453,-79.4775,43.7495,-79.3199);<;);out meta;\n",
    "\n",
    "![title](img/toronto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered in the Map\n",
    "\n",
    "During the data wrangling process, I validated and corrected the following values: \n",
    "\n",
    "1- Country  \n",
    "The code validates that the country value for all addresses is \"CA\". During this validation there was no address found with an invalid value for country field.\n",
    "\n",
    "2- Province  \n",
    "The code validates the province value as well. During the audit, I realized that people used different values for province and some of them had typos. So I came up with the list of values that could be replaced by the value that I wanted to have as the province that is \"ON\". Here is the list of to be replaced values:  \n",
    "ON: \"Ontario\", \"ontario\",\"On\", \"on\", \"Onatrio\".  \n",
    "\n",
    "3- City  \n",
    "Same for validating the city. The values I wanted to have in my dataset were \"Toronto, York, North York, East York\". And as for the province, I discovered other possible values people used to name the cities and here is the list of to be replaced values for the cities:  \n",
    "Toronto: \"CityofToronto\", \"City of Toronto\", \"Toronto,ON\", \"Torontoitalian\"  \n",
    "North York: \"NorthYork\"  \n",
    "East York: \"EastYork\"  \n",
    "\n",
    "4- Postal Code  \n",
    "The valid postal code format in Canada is [A-Z][0-9][A-Z][ ][0-9][A-Z][0-9]. I audited all the postal code values and I found number of invalid postal codes like A \"M36 0H7\". I removed these values but I also found that there were a lot of postal codes without the space in between. So I corrected them and I included them in the dataset.  \n",
    "\n",
    "5- Street  \n",
    "While auditing the street names, I wanted to remove all the abbreviations. So I searched for any kind of value followed by \".\". I found a lot of \"St.\" values in the dataset. I studied more and I figured that this abbreviation in lots of cases is being used for \"Saint\" and not \"Street\". So I came up with the list of names (saints) that could come after st. abbreviation and here is the list:  \n",
    "\"Andrew's\", \"Edmund's\", \"John's\", \"Leonard's\", \"George\", \"Clair\", \"Helens\", \"Dennis\", \"Joseph\", \"Clarens\", \"Matthews\", \"Hubert\", \"James\", \"David\", \"Thomas\", \"Mary\", \"Patrick\", \"Clements\", \"Leonards\", \"Hildas\", \"Mathias\", \"Andrews\", \"Raymond\", \"Annes\", \"Cuthberts\", \"Cuthberts\", \"Ives\", \"Edmunds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "#find the two-level values \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "#find the three-level values \n",
    "lower_double_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "#find the problamatic values \n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#valid country  \n",
    "canada = re.compile(r'^(CA)$')\n",
    "\n",
    "#valid province\n",
    "ontario = re.compile(r'^(ON)$')\n",
    "\n",
    "#other province values to be converted to \"ON\"\n",
    "ontario_alternative = [\"Ontario\", \"ontario\",\"On\", \"on\", \"Onatrio\"]\n",
    "\n",
    "#valid cities\n",
    "city = re.compile(r'^(Toronto|York|North York|East York)$')\n",
    "\n",
    "#other toronto values to be converted to \"Toronto\"\n",
    "toronto_alternative = [\"CityofToronto\", \"City of Toronto\", \"Toronto,ON\", \"Torontoitalian\"]\n",
    "\n",
    "#other North York values to be converted to \"North York\"\n",
    "northyork_alternative = [\"NorthYork\"]\n",
    "\n",
    "#other East York values to be converted to \"East York\"\n",
    "eastyork_alternative = [\"EastYork\"]\n",
    "\n",
    "#valid postal code format\n",
    "postcode = re.compile(r'^([A-Z][0-9][A-Z][ ][0-9][A-Z][0-9])$')\n",
    "\n",
    "#other postal code format to be converted to the valid one\n",
    "postcode_alternative = re.compile(r'^([A-Z][0-9][A-Z][0-9][A-Z][0-9])$')\n",
    "\n",
    "#any type of abbreviation\n",
    "street = re.compile(r'\\b([a-z]|_)*\\.', re.IGNORECASE)\n",
    "\n",
    "#Created object \n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "#Position object\n",
    "POSITION = [ \"lat\", \"lon\"]\n",
    "\n",
    "#valid names to come after St.\n",
    "SAINTS = [\"Andrew's\", \"Edmund's\", \"John's\", \"Leonard's\", \"George\", \"Clair\", \"Helens\", \"Dennis\", \"Joseph\", \"Clarens\",\n",
    "          \"Matthews\", \"Hubert\", \"James\", \"David\", \"Thomas\", \"Mary\", \"Patrick\", \"Clements\", \"Leonards\", \"Hildas\", \"Mathias\",\n",
    "          \"Andrews\", \"Raymond\", \"Annes\", \"Cuthberts\", \"Cuthberts\", \"Ives\", \"Edmunds\"]\n",
    "\n",
    "#dictionary of all problamtic items by category\n",
    "problematic_elements = {}\n",
    "problematic_elements[\"key_invalid_char\"] = []\n",
    "problematic_elements[\"key_double_colon\"] = []\n",
    "problematic_elements[\"country\"] = []\n",
    "problematic_elements[\"province\"] = []\n",
    "problematic_elements[\"city\"] = []\n",
    "problematic_elements[\"street\"] = []\n",
    "problematic_elements[\"postcode\"] = []\n",
    "\n",
    "#create json object based on element\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node[\"id\"] = element.attrib[\"id\"]\n",
    "        node[\"type\"] = element.tag\n",
    "        for key,value in element.attrib.iteritems():\n",
    "            if key in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node[\"created\"][key] = value\n",
    "            elif key in POSITION:\n",
    "                if 'pos' not in node:\n",
    "                    node['pos'] = [None]*2\n",
    "                if key == 'lat':\n",
    "                    node[\"pos\"][0] = float(value)\n",
    "                elif key == 'lon':\n",
    "                    node[\"pos\"][1] = float(value)\n",
    "            else:\n",
    "                node[key] = value\n",
    "            \n",
    "        for tag in element.iter():\n",
    "            if tag.tag == 'tag':\n",
    "                tag_key = tag.attrib[\"k\"]\n",
    "                if is_match(problemchars, tag_key):\n",
    "                    problematic_elements[\"key_invalid_char\"].append(tag_key)\n",
    "                    continue\n",
    "                if is_match(lower_double_colon , tag_key):\n",
    "                    problematic_elements[\"key_double_colon\"].append(tag_key)\n",
    "                    continue\n",
    "                if is_match(lower_colon , tag_key):\n",
    "                    parse_colon(node, tag, tag_key)\n",
    "                else:\n",
    "                    is_valid_value,final_value = check_value(tag_key, tag.attrib['v'])\n",
    "                    if is_valid_value:\n",
    "                        node[tag_key] = final_value\n",
    "            elif tag.tag == 'nd':\n",
    "                if 'node_refs' not in node:\n",
    "                    node['node_refs'] = []\n",
    "                node['node_refs'].append(tag.attrib['ref'])\n",
    "       \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# create two level objects like address\n",
    "def parse_colon(node, element, tag_key):\n",
    "    key_part_1,key_part_2 = tag_key.split(':')\n",
    "    if key_part_1 not in node:\n",
    "        node[key_part_1] = {}\n",
    "    elif type(node[key_part_1]) is not dict:\n",
    "        type_name = node[key_part_1]\n",
    "        node[key_part_1] = {}\n",
    "        node[key_part_1][\"type\"] = type_name\n",
    "    is_valid_value,final_value = check_value(tag_key, element.attrib['v'])\n",
    "    if is_valid_value:\n",
    "        node[key_part_1][key_part_2] = final_value\n",
    "\n",
    "# validate and convert value based on key. it return a tuple of the following\n",
    "# boolean : indicating whether the validation was successful\n",
    "# string : original or converted value, Empty string if validation fails \n",
    "def check_value(key, value): \n",
    "    if key == 'addr:country':\n",
    "        m = canada.search(value)\n",
    "        if m:\n",
    "            return True,value\n",
    "        problematic_elements[\"country\"].append(value)\n",
    "        return False,\"\"\n",
    "    if key == 'addr:province':\n",
    "        m = ontario.search(value)\n",
    "        if m:\n",
    "            return True,value\n",
    "        elif value in ontario_alternative:\n",
    "            return True,\"ON\"\n",
    "        problematic_elements[\"province\"].append(value)\n",
    "        return False,\"\"\n",
    "    if key == 'addr:city':        \n",
    "        m = city.search(value)\n",
    "        if m:\n",
    "            return True,value\n",
    "        elif value in toronto_alternative:\n",
    "            return True,\"Toronto\"\n",
    "        elif value in northyork_alternative:\n",
    "            return True,\"North York\"\n",
    "        elif value in eastyork_alternative:\n",
    "            return True,\"East York\"\n",
    "        problematic_elements[\"city\"].append(value)\n",
    "        return False,\"\"\n",
    "    if key == 'addr:postcode':        \n",
    "        m = postcode.search(value)\n",
    "        if m:\n",
    "            return True,value\n",
    "        ma = postcode_alternative.search(value)\n",
    "        if ma:\n",
    "            return True,value[:3]+\" \"+value[3:]\n",
    "        problematic_elements[\"postcode\"].append(value)\n",
    "        return False,\"\"\n",
    "    if key == 'addr:street':        \n",
    "        m = street.search(value)\n",
    "        if m:\n",
    "            saint_name = value[4:].split(' ')\n",
    "            if value[0:4] == 'St. ' and saint_name[0] in SAINTS:\n",
    "                return True, \"Saint \" + value[4:]\n",
    "            problematic_elements[\"street\"].append(value)\n",
    "            return False,\"\"\n",
    "    \n",
    "    return True,value\n",
    "\n",
    "#match a regular expression with an item\n",
    "def is_match(rg,item):\n",
    "    m = rg.search(item)\n",
    "    if m:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#main function\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "data = process_map('toronto.osm', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "Here is the overview statistics of the dataset\n",
    "\n",
    "toronto.osm      105,444KB  \n",
    "toronto.osm.json 146,656KB  \n",
    "\n",
    ">db.toronto.find().count()  \n",
    "474579  \n",
    "\n",
    ">db.toronto.find({\"type\":\"node\"}).count()  \n",
    "396867  \n",
    "\n",
    ">db.toronto.find({\"type\":\"way\"}).count()  \n",
    "77589  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other queries\n",
    " \n",
    "Total number of street names with the word \"Saint\" in it:  \n",
    ">db.toronto.find({\"addr.street\": /Saint/}).count()  \n",
    "1068\n",
    "\n",
    "Top 10 contributing users:  \n",
    ">db.toronto.aggregate([{\"\\$group\":{\"_id\" : \"\\$created.user\",\"count\" : {\"\\$sum\":1}}}, {\"\\$sort\" : {\"count\":-1}},{\"\\$limit\":10}])  \n",
    "{ \"_id\" : \"andrewpmk\", \"count\" : 360571 }  \n",
    "{ \"_id\" : \"Kevo\", \"count\" : 36098 }  \n",
    "{ \"_id\" : \"Mojgan Jadidi\", \"count\" : 14407 }  \n",
    "{ \"_id\" : \"andrewpmk_imports\", \"count\" : 6254 }  \n",
    "{ \"_id\" : \"TristanA\", \"count\" : 5451 }  \n",
    "{ \"_id\" : \"Nate_Wessel\", \"count\" : 4953 }  \n",
    "{ \"_id\" : \"Shrinks99\", \"count\" : 2748 }  \n",
    "{ \"_id\" : \"Nate_Wessel (consulting)\", \"count\" : 2514 }  \n",
    "{ \"_id\" : \"lorandr_telenav\", \"count\" : 2423 }  \n",
    "{ \"_id\" : \"Bootprint\", \"count\" : 2138 }  \n",
    "\n",
    "Top 10 banks in the area:  \n",
    ">db.toronto.aggregate([{\"\\$match\":{\"amenity\":{\"\\$exists\":1}, \"amenity\":\"bank\"}}, {\"\\$group\":{\"_id\":\"\\$name\", \"count\":{\"\\$sum\":1}}},{\"\\$sort\":{\"count\":-1}}, {\"\\$limit\":10}])  \n",
    "{ \"_id\" : \"TD Canada Trust\", \"count\" : 60 }  \n",
    "{ \"_id\" : \"Scotiabank\", \"count\" : 43 }  \n",
    "{ \"_id\" : \"CIBC\", \"count\" : 34 }  \n",
    "{ \"_id\" : \"BMO Bank of Montreal\", \"count\" : 29 }  \n",
    "{ \"_id\" : \"RBC\", \"count\" : 14 }  \n",
    "{ \"_id\" : \"RBC Royal Bank\", \"count\" : 14 }  \n",
    "{ \"_id\" : \"RBC Financial Group\", \"count\" : 11 }  \n",
    "{ \"_id\" : \"CIBC Banking Centre\", \"count\" : 10 }  \n",
    "{ \"_id\" : \"Bank of Montreal\", \"count\" : 9 }  \n",
    "{ \"_id\" : \"HSBC\", \"count\" : 7 }  \n",
    "\n",
    "Total number of address in each city\n",
    ">db.toronto.aggregate([{\"\\$group\":{\"_id\":\"\\$addr.city\",\"count\":{\"\\$sum\":1}}}])  \n",
    "{ \"_id\" : \"York\", \"count\" : 239 }  \n",
    "{ \"_id\" : \"North York\", \"count\" : 942 }  \n",
    "{ \"_id\" : \"Toronto\", \"count\" : 41544 }  \n",
    "{ \"_id\" : \"East York\", \"count\" : 750 }  \n",
    "{ \"_id\" : null, \"count\" : 431104 }  \n",
    "\n",
    "Top 10 cuisines in the area\n",
    ">db.toronto.aggregate([{\"\\$match\":{\"amenity\":{\"\\$exists\":1}, \"amenity\":\"restaurant\"}}, {\"\\$group\":{\"_id\":\"\\$cuisine\", \"count\":{\"\\$sum\":1}}},{\"\\$sort\":{\"count\":-1}}, {\"\\$limit\":10}])  \n",
    "{ \"_id\" : null, \"count\" : 571 }  \n",
    "{ \"_id\" : \"japanese\", \"count\" : 38 }  \n",
    "{ \"_id\" : \"italian\", \"count\" : 27 }  \n",
    "{ \"_id\" : \"indian\", \"count\" : 26 }  \n",
    "{ \"_id\" : \"chinese\", \"count\" : 22 }  \n",
    "{ \"_id\" : \"sushi\", \"count\" : 19 }  \n",
    "{ \"_id\" : \"thai\", \"count\" : 17 }  \n",
    "{ \"_id\" : \"asian\", \"count\" : 14 }  \n",
    "{ \"_id\" : \"mexican\", \"count\" : 13 }  \n",
    "{ \"_id\" : \"pizza\", \"count\" : 12 }  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas\n",
    "As described before, I kept the problematic entries by their category in the problematic_elements dictionary. Looking at the problematic province and city entries before replacing the faulty values, I had 168 invalid province entry and 34081 invalid city entry.  \n",
    ">print len(problematic_elements[\"province\"])  \n",
    "print len(problematic_elements[\"city\"])  \n",
    "168  \n",
    "34081  \n",
    "\n",
    "Thinking about that, and I noticed that these kind of inconsistency could have been prevented during the data entry. For example, OpenStreetMap could detect that the user is entering data for Toronto region and give user an option to reuse predefined values for country, province and city. The benefit of proving this kind of predefined value is that it decreases the chance of human error during the data entry and also it could eventually define a standard way of contributing to OpenStreetMap for different entities like address. The downside however is that predefined values themselves would need maintenance. They might not be complete or there might be inconsistency or even error in them. So we will need another process to audit these values."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
